{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Notes\n",
    "\n",
    "\n",
    "\n",
    "These are some concise notes representing the most important ideas discussed.\n",
    "\n",
    "--------\n",
    "\n",
    "## MDP: RL Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Reinforcement Learning\n",
    "\n",
    "- Unlike DP, RL does NOT assume any knowledge of the world dynamics. It instead relies on samples to find optimal policy.\n",
    "- The **Conventional Approach** to Option Pricing requires building a model of the world by designing a stochastic process and then caliberating it to aution and stock pricing data.\n",
    "- **Model Calibration**: is the idea of formulating the law of dynamics by estimating model parameters. This amounts to minimization of some loss function between what's observed and model outputs.\n",
    "\n",
    "\n",
    "- **RL** on the other hand focuses on the original taks of finding an optimal price and hedge by relying on data samples instead of a model.\n",
    "\n",
    "- **Batch RL** is an off-line RL, where agent mainly relies on some histoically collected data.\n",
    "\n",
    "\n",
    "- **Vladimir Vapnik** formulated the principle: \"one should avoid solving more difficult intermediate problems when solving a target problem.\"\n",
    "\n",
    "\n",
    "\n",
    "- Unlike DP, RL does NOT need to know reward and transition probability functions, as it relies on samples.\n",
    "\n",
    "- The information set for RL includes a tuplet $(X_t^{(n)}, a_t^{(n)}, R_t^{(n)}, X_t+1^{(n)}$ for each step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Approximations\n",
    "\n",
    "\n",
    "- Again, we try to approximately solve Bellman Optimality Eq: $$Q_t^*(x,a) = \\mathbb{E}[R_t + \\gamma \\max_{a\\in A} Q_{t+1}^*(X_{t+1}|x,a)]$$\n",
    "- Optimal Q-fn value is equal to the expected optimal rewards, plus the discounted expected value of the next step optimal Q-fn.\n",
    "- Expectation here is one-step expectation, involving next step quantities being conditional on the information available at time T.\n",
    "\n",
    "\n",
    "- For a *discrete* state model, we simply sum up over all possible next states with the corresponding transition probabilities as weights in this sum.\n",
    "- For a *continuous* state model, the calculation of expectation involves integrals instead of sums.\n",
    "- We replace expectations by their empirical averages.\n",
    "\n",
    "\n",
    "- We already know how the conventional DP would work in such setting. Yet with RL we observe stock prices in the rewards, and mainly rely on empirical means to estimate the Right Hand of Bellman Optimality. Hence, estimating optimal Q-values.\n",
    "\n",
    "- The **Robbins-Monro** Alg estimates the mean without directly summing the samples, but instead adding data points one by one, and iteratively updating the running estimation of the mean $\\hat{x}_k$ (where $k$ is the number of iterations, or the number of data points in a dataset): $$\\hat{x}_{k+1}=(1-\\alpha_k)\\hat{x}_k + \\alpha_k x_k$$\n",
    "- The advantage of such approach is it actually does converge to a true mean with probability of one under certain conditions.\n",
    "\n",
    "\n",
    "\n",
    "- Robbins-Monro Alg can be used for both on-line and batch-mode setting.\n",
    "- The optimal choice of learning rate in the Robbins-Monroe Alg is NOT universal but depends on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "\n",
    "- As per Watkins, Q-Learning works only in a setting called discrete states and discrete actions. We say Q-fn is in tabular form when using one value of Q-fn for each combination of states and actions.\n",
    "\n",
    "- Q-Learning converges to the true optimal action-value fn with probability of one, given enough data.\n",
    "\n",
    "- The optimal Q-fn is learnt in Q-Learning iteratively, where each step (Q-Iteration) implements one iteration of the Robbins-Monro Alg.\n",
    "\n",
    "- Q-Learning is obtained by using the Robbins-Monro Stochastic Approximation to estimate the unknown expectation in Bellman Optimality. \n",
    "\n",
    "\n",
    "$$Q_{t,k+1}^*(X_t,a_t)=(1-\\alpha_k)Q_{tk}^*(X_t,a_t)+\\alpha_k[R_t(X_t,a_t,X_{t+1}+\\gamma \\max_{a\\in A}Q{t+1,k}^*(X_{t+1},a) )]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitted Q-Iteration\n",
    "\n",
    "\n",
    "- For Monte Carlo path, historical path of the stock was used simultaneously when calculating an optimal policy. The problem however was that the classical Q-Learning will too long to converge. We aim for something faster.\n",
    "\n",
    "- Most popular extension of of Q-Learning to Batch RL setting is what's called **Fitted Q Iteration**. It was mainly developed for time-stationary problems, where Q-fn does NOT depend on time. Yet it can be also applied for both discrete and continuous state-action spaces.\n",
    "\n",
    "- FQI works by using all Monte Carlo paths for the replication portfolio simultaneously. We use the same set of basis functions ${\\Phi_n(x)}$ as we did with DP.\n",
    "- Optimal Q-fn $Q_t^*(X_t,a_t)$ is a quadratic function of $a_t$. It can be written as an expansion in basis functions, with time-dependent coefficient matrix $\\mathbf W_t$:\n",
    "\n",
    "\n",
    "$$Q_t^*(X_t,a_t)= A_t^T \\mathbf W_t \\Phi(X_t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitted Q-Iteration: the $\\epsilon$-basis\n",
    "\n",
    "- Here we just want to convert matrix $W_t$ to a vector. So we are going to rearrange the previous equation, converting it into a product of a parameter vector and a vector that depends on both the state and the action:\n",
    "\n",
    "\n",
    "$$Q_t^*(x,a)=A_t^T \\mathbf W_t \\Phi (X)$$\n",
    "\n",
    "\n",
    "$$ = \\vec{\\mathbf W}_t \\vec{\\Psi} (X_t,a_t) $$\n",
    "\n",
    "\n",
    "- $\\vec{W}_t$ is obtained by concatenating columns of matrix $\\mathbf W_t$ while vec $ vec \\left( {\\bf \\Psi} \\left(X_t,a_t \\right) \\right) = \n",
    "  vec \\, \\left( {\\bf A}_t  \\otimes {\\bf \\Phi}^T(X) \\right) $ stands for \n",
    "a vector obtained by concatenating columns of the outer product of vectors $ {\\bf A}_t $ and $ {\\bf \\Phi}(X) $.\n",
    "\n",
    "\n",
    "- For **RL**:\n",
    "    - both parameter vec $vec{\\mathbf W}_t$ and state-action basis $vec{\\Psi}_t$ have 3M components.\n",
    "    - number of data records per time step is 3N $(X_t,a_t,R_t)$.\n",
    "    \n",
    "- For **DP**:\n",
    "    - 2M parameters.\n",
    "    - only N values of $X_t$ as input data.\n",
    "\n",
    "- Counting by the number of parameters to learn, the RL setting has more unknowns, but also a higher dimensionality of data (more data per observation) than DP setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitted Q-Iteration at Work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Coefficients $\\mathbf W_t$ shall be computed recursively backward in time as before.\n",
    "- One step **Bellman Iteration** can be interpreted as Regression: $$R_t(X_t,a_t,x_{t+1}) + \\gamma \\max Q_{a \\in A}^*(X_{t+1},a)$$\n",
    "\n",
    "$$      = \\vec{\\mathbf W}_t \\vec{\\mathbf \\Psi}(X_t,a_t) + \\epsilon_t $$\n",
    "    \n",
    "    \n",
    "- For **DP**:\n",
    "    - Q-fn is expanded in the state basis $\\Phi_t$.\n",
    "    - applies only at optimal Q-fn for optimal action $a_t^*$\n",
    "    - rewards $R_t$ are computed\n",
    "\n",
    "- For **RL**:\n",
    "    - Q-fn is expanded in the state-action basis $\\vec{\\Psi}_t$\n",
    "    - works for any action $a_t$, not just optimal actions\n",
    "    - both rewards $R_t$ and actions $a_t$ are observed\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "- In the DP and RL solutions, Q-fn is an expansion in a set of state-action basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Solution: Discussion and Examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

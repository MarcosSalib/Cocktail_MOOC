{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Notes\n",
    "\n",
    "\n",
    "\n",
    "These are some concise notes representing the most important ideas discussed.\n",
    "\n",
    "--------\n",
    "\n",
    "## Part 1: MDP for Discrete-Time BS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Formulation\n",
    "\n",
    "- We aim to reformulate the discrete-time BSM as MDP model, where the *system* being controlled is a hedge portfolio, and *control* is a stock position.\n",
    "\n",
    "- We solve the problem by a **sequential maximization of rewards** (negatives of hedge portfolio one-step variance multiplied by the risk aversion $\\lambda$, plus a drift term).\n",
    "\n",
    "- *In principle*, we can consider either discrete state or continuous state problems. Continuous state formulation is practically irrelevant.\n",
    "\n",
    "\n",
    "- **State Variable**: instead of using stock prices $S_t$, we shall used $X_t$ which is varied function of $S_t$. $dX_t$ shall be a standard Brownian Motion scaled by volatility $\\sigma$.\n",
    "\n",
    "- Actual hedging decision $a_t(x_t)$ are determined by a time-dependent policy $\\pi(t, X_t)$. We consider such policy determinstic.\n",
    "\n",
    "- Notion of determinstic policy $\\pi$ is mapping state $X_t$ and time $T$ into action $A_t$\n",
    "\n",
    "- **Value function** $V_t$ is defined as the negative of option price. More on the equations in the corresponding notebook.\n",
    "\n",
    "\n",
    "- **Bellman Equation** for such model is \n",
    "$$V_t^\\pi(X_t) =  \\mathbb{E}_t^\\pi [R(X_t, a_t, X_{t+1}) + \\gamma V_{t+1}^\\pi(X_{t+1})] $$\n",
    "\n",
    "- Here $R(X_t, a_t, X_{t+1})$ is a one-step time-dependent random reward. \n",
    "- The expected reward in the present MDP is quadratic in action $a_t$.\n",
    "\n",
    "**N.B.** when $\\lambda \\to 0$, the expected reward becomes linear in $a_t$, so it does NOT have maximum i.e. there is no risk aversion.\n",
    "In this framework, quadratic risk is incorporated in a standard (risk-neutral) MDP formulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Value Function\n",
    "\n",
    "- As usual the optimal policy $\\pi_t^*$ is determined as policy that maximizes the value function $V_t^\\pi$.\n",
    "- Needless to say, the optimal value fn also satisfies the Bellman Optimality Equation.\n",
    "\n",
    "- If system dynamics are known, we can solved them using Dynamic Programming.\n",
    "- If system dynamics are unknown, optimal policy should be computed using *samples*.\n",
    "\n",
    "\n",
    "- The **Action-Value Fn** is defined by an expectation of the same expression as in the definition of the value fn, but conditioned on both the current state $X_t$ and the initial actions $a=a_t$, while following policy $\\pi$ afterwards.\n",
    "\n",
    "- **Bellman Eq** for Q-fn: $$Q_t^\\pi(X,a) =  \\mathbb{E}_t[R(X_t, a_t, X_{t+1}|x,a] + \\gamma \\mathbb{E}_t\\pi [V_{t+1}^\\pi(X_{t+1})|x] $$\n",
    "\n",
    "\n",
    "- **Optimal Value fn** and **Optimal Q-fn** are related as $V_t^*(x)=max_a(Q_t^*(x,a))$\n",
    "\n",
    "- A *greedy* policy always maximize the current-time Q-fn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Action from Q-Fn\n",
    "\n",
    "\n",
    "- The BS results can be recovered from the DP formulation in the limit $\\lambda \\to 0 $ and $\\Delta t \\to 0 $.\n",
    "\n",
    "- In the **DP** model, hedging comes ahead of pricing. In the **BS** model, it is the other way around.\n",
    "\n",
    "\n",
    "- The quadratic hedging of the **discrete-time BSM** model only looks at risk of a hedge portfolio. However here  the expected reward has both a drift and variance parts, similar to the Markowitz risk-adjusted portfolio return analysis.\n",
    "\n",
    "- *Hence,* The optimal hedge in an **MDP** model differs from the optimal hedge in the **risk minimization** approach. The former's objective fn is based on a risk-adjusted return, while for the later it is based purely on risk.\n",
    "\n",
    "- For a pure risk-focused quadratic hedge, we can set $\\mu = r$ or $\\lambda \\to \\infty$ in the $a_t^*(X_t)$ eq (48). In general, such eq gives hedges that can be applied for both hedgin and investment with options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Recursion for Q-Star\n",
    "\n",
    "- Since we have the analytical formula for optimal action $a_t^*$, we can do the backward recursion from $T-1$ to $0$ directly.\n",
    "\n",
    "- **N.B.** as the backward recursion is applied directly to the optimal Q-fn $Q_t(S_t, a_t^*)$, neither continuous nor discrete action space representation is required in our setting, as the action in this equation is always just one optimal action.\n",
    "\n",
    "\n",
    "- The *ask* price becomes a negative of the Q-fn: $$C_t^{ask}(S_t) = - Q_t(S_t, a_t^*)$$\n",
    "\n",
    "\n",
    "- In the **DP** formulation, both optimal price and hedge are parts of the same value $Q_t^*(X_t,a_t)$. In **BS** model, we have 2 separate formulas for the price and the hedge.\n",
    "\n",
    "- In **DP**, hedgin comes ahead of pricing, while in **BS** it is the other way around.\n",
    "\n",
    "\n",
    "- Vanishing optimization:\n",
    "    - a quadratic objective fn is the **DP** setting with $\\lambda>0$, $\\Delta t>0$. The link between price and hedge is explicit.\n",
    "    - In case $\\lambda>0$, no quadratic optimization in the DP sense.\n",
    "    - If both $\\lambda=0$ and $\\Delta t=0$, then risk is lost i.e. nothing to optimize anymore.\n",
    "\n",
    "\n",
    "**N.B.** having both $\\lambda>0$ and $\\Delta t>0$, the DP method gives a consistent hedging and pricing scheme that takes into account residual risk that persists in options under discrete hedging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Part 2: Monte Carlo Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Functions\n",
    "\n",
    "- The **optimal hedge**: $$a_t^*(X_t)= \\frac{\\mathbb{E}_t [\\Delta \\hat{S}_t {\\Pi}_{t+1} + \\frac{1}{2\\sigma\\lambda}\\Delta S_t]}{\\mathbb{E}_t [(\\Delta \\hat{S}_t)^2] }$$\n",
    "\n",
    "- For the discrete state formulation, there is a finite set of nodes ${\\{X_n\\}}_{n=1}^M$, with values $Q_n$ of the optimal Q-function at these nodes. $$Q(X) = \\sum_{n=1}^M Q_n \\delta _x, x_n$$\n",
    "\n",
    "- We shall replace the kronecker symbol with \"one-hot\" basisi function $\\Phi_n(X)$, where it is equal to 1 whenever $X=X_n$ and 0 elsewhere.\n",
    "\n",
    "\n",
    "- For our convenience now, we shall be using **B-Splines** as Basis Functions. B-Splines are non-negative, integrate to one, and $B_{i,n}$ is only non-zero on the interval $[x_i, x_{i+n+1}]$\n",
    "\n",
    "**N.B.** A cubic B-spline is a piece-wise local polynomial of a third degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Hedge with Monte Carlo\n",
    "\n",
    "- **Idea:** is using all MC paths simultaneously to learn optimal actions. Learning optimal actions of all states simultaneously means learning a policy, which is basically our objective.\n",
    "\n",
    "- Our optimal equations will be functions of the basis:\n",
    "    - Optimal Action (Hedge) $$a_t^*(X_t) = \\sum_n^M \\phi_{nt} \\Phi_n(X_t) $$\n",
    "    - Optimal Q-Fn: $$Q_t^*(X_t,a_t^*) = \\sum_n^M \\omega_{nt} \\Phi_n(X_t) $$\n",
    "    \n",
    "    \n",
    "- The coefficients $\\phi_{nt}$ and $\\omega{nt}$ are computed recursively backward in time for $t=T-1, ..., 0$.\n",
    "\n",
    "- More about the full explanation about how to get coefficient $\\phi_t^*$ can be found in the notebook.\n",
    "\n",
    "\n",
    "- The **DP** solution expands both the optimal action and optimal Q-fn in the same set of basis functions.\n",
    "\n",
    "**N.B.** The optimal policy in the DP solution is computed by analyzing all MC paths simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Q-fn with Monte Carlo\n",
    "\n",
    "- Once the coefficient $\\phi_t^*$ of the optimal action $a_t^*$ are found, we shift to focus on finding coefficients $\\omega_{nt}$.\n",
    "\n",
    "- For **Bellman optimality** equation, the optimal action $a_t^*$ can be interpreted as regression of form: $$R_t(X_t,a_t^*,X_{t+1}) + \\gamma max_{a_{t+1} \\in\\mathcal{A}} Q_{t+1}^* (X_{t+1}, a_{t+1})$$\n",
    "which is equal to $$Q_t^*(X_t, a_t^*) + \\mathcal{E}_t$$\n",
    "\n",
    "- $\\mathcal{E}_t$ is a random noise at time t with zero mean.\n",
    "\n",
    "- **Reward** is then computed from simulated paths: $$R_t = \\gamma\\Pi_{t+1} - \\Pi_{t} - \\lambda Var_t[\\Pi_{t}]$$\n",
    "\n",
    "\n",
    "- The coefficients $\\omega_{nt}$ can then be found by solving **least-square optimization** problem.\n",
    "\n",
    "\n",
    "\n",
    "- Another 2 pairs are also introduced matrix $C_t$ and vector $D_t$.\n",
    "- Full summary about **MC Backward Recursion** in the notebook.\n",
    "\n",
    "\n",
    "- **Note** \n",
    "  - Coefficients of expansion of the Q-fn in basis function are obtained in the DP solution from the Bellman equation interpreted as a regression problem which is solved using Least Square Minimization.\n",
    "  - The DP solution computes rewards as a part of hedge optmization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}



some notes..


[Neural Networks]

* Logistic Regression can be viewed as a Neural Network with just one sigmoid neuron.
* Linear Regression can be viewed as a Neural Network with just one "linear neuron" (a node with a linear activation function).
* Deep Neural Networks are obtained when there are more than two hidden layers.


[Gradient Descent Optimization]

* A good choice of the learning rate is important: if the learning rate is too small, it takes long for the alg to converge, but if it is too high, the alg may diverge.
* GD has one free parameter called learning rate.
* Making the learning rate variable larger initially in the training, and smaller as the training progresses, may accelerate convergence.


[GD for NN]

* The Backpropagation alg for NN amounts for GD applied to the train error, with a reverse-mode autodiff for a recursive calculation of all derivatives.
* 2 main problems with GD in general:
	- it needs to iterate over the full dataset, for one training step.
	- it finds local,  not global, minimum.


[Stochastic GD]

* SGD mainly allows for faster updates, by taking mini batches instead.
* As mini-batches are randomly drawn from a data-generating distribution, SGD minimizes the generalization error plus some random noise.
* SGD offers better generalization, by reducing generalization loss.
* Due to stochasticity of the mini-batch selection, it helps escaping local minimia of the training loss function.

N.B. Such equation is known in physics as Langevin Equation, describing the stochastic relaxation of some physical quantities subject to external force plus random noise.
As we keep iterating, the error term decrease and the noise starts to dominate.

* SGD attempts at a direct minimization of the generalization error, by producing samples from a data generating distribution in the form of mini-batches.
* The Backpropagation alg for NN amounts to GD applied to the training error, with a reverse-mode autodiff for a recursive calculation of all derivatives.
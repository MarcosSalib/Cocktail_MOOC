

some notes...


[Bias-Veriance Tradeoff]

* BIAS is the square of expected difference of approx predictor from the ture predictor.
* VARIANCE is the sensitivity of predicted value to the choice of dataset.
* NOISE is a property of data, beyond our control. It does NOT depend on true or predicted values.

* The tradeoff comes from the fact that: complex models with many features tend to have low bias and high variance. On the other hand, simple models with less features tend to have high bias and low variance.
* Assuming you have a complex model with low bias and high variance, one way to reduce variance is to try to bound somehow the values of model parameters, so the model outputs would vary less with a variation of the input data.
* The importance of such tradeoff lies from the fact that such tradeoff controls model complexity.


[No Free Lunch Theorem]

* No single ML classification alg can be universally better than any other one, on all domains.
* More restrictly, all classifiers have the same error rate when averaged over all possible data-generating distributions.
* Informally, it is NOT the size of the dog in the fight that matters.

* The theorem states that all classifications alg have exactly the same out-of-sample error rate when averaged over all possible data.


[Overfitting & Model Capacity]

* Our assumption is always that the samples from a data-generating distribution are independent identically distributed iid.
* A test set is used to detect when our model starts to overfit data. This is done by estimating a generalization error "test error".

* 	- UNDERFITTING is where the bias is high.
	- OVERFITTING is where the variance is high. It is where in-sample errors are small, but out-of-sample errors are large.
* Ideally, we aim to balance between model complexity and data complexity.


[Model Complexity - Data Complexity]

- low,low:	linear architectures / parametric models.
- high,high:	nonlinear architectures / non-parametric models.
- low,high:	typical underfitting case, as the model is too simple to handle the data.
- high,low:	obvious overfitting case, the model is too good!

* In general, capacity is controlled by the choice of a hypothesis space (architecture), and other techniques like Regularization, Dimensionality Reduction, Bayesian Probability, etc.


[Linear Regression]

* The task is to predict a scalar value from a vector of predictions "Features", given a dataset "Design Matrix".
* Our performance measure here is Mean Squared Error (MSE) on test set.
* We optimize parameters, by minimizing MSE on training set. This shall be done by setting the gradient of MSE_train to zero.
 N.B. Both MSE are used to estimate the same generalization (expected) error from the empirical distribution.


[Hat Matrix]

* The resultant optimal weight will be equal to H * y, where H is the projection matrix.
* The projection matrix is NOT orthogonal, being idempotent, its square is equal to the matrix itself.

* It projects the dependent variable in regression onto the predicted variable.

(Degenerate Matrix)

* Is just another word for SINGULAR matrices.
* In case our data matriX X has correlated columns, this might led the matrix to be degenerate.
* Another case is when the columns are linear dependent "multi collinearity", where the determinant is close zero. Generally leading to instability.


[Regularization]

* Idea is to modify the objective function of minization of MSE_train so that MSE_test shall have small variance.
* Note. the regularization term does NOT include any data X, only parameters W.
* Having large lambda, the focus will be only on the regularization term, NOT on MSE_train

* Common choices for the regularizer:
	- L1 Reg: penalizing large weights
	- L2 Reg: enforces sparsity
	- Entropy Reg: motivated by Bayesian Statistics.


[Hyperparameters]

* They are any quantitative features of the model that are NOT directly optimized by minimizing the in-sample loss e.g. MSE_train
* They are mainly reponsible for controlling model complexity.

* Choosing hyperparameters is done by:
	- Splitting the dataset into training, validation and testing sets. Training set for model parameters and Validation set for hyperparameters. Testing set is only used once for final testing. One drawback for such method is that it is not ideal with small datasets.
	- Cross-Validation. You know how.
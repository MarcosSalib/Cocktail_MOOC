{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Notes\n",
    "\n",
    "\n",
    "\n",
    "These are some concise notes representing the most important ideas discussed.\n",
    "\n",
    "--------\n",
    "\n",
    "## Part 1: Markov Decision Processes MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes\n",
    "\n",
    "- Markov processes are characterized by short memory, the future depends NOT on all history, but on the current state.\n",
    "N.B. The environment we are considering is fully-observable.\n",
    "- The goal is to maximize the expected total reward by choosing an optimal policy.\n",
    "\n",
    "### Risk in RL\n",
    "\n",
    "- **Risk-Neutral RL** looks only at a mean of the distribution of total reward, it does NOT look at risk of a given policy.\n",
    "- **Risk-Sensitive RL** looks at risk of some higher moments of the resulting distribution of cumulative rewards. That's in addition to its mean value as is done in a conventional Risk-Neutral RL.\n",
    "\n",
    "### Decision Policies\n",
    "\n",
    "- 2 types: \n",
    "\t- **Deterministic**: can be expressed as a function -usually linear- of states. It is a policy that gives a fixed action for each state.\n",
    "\t- **Stochastic**: expressed as a probability distribution. It aims to generalize DETERMINSTIC policies, taking different actions for the same state.\n",
    "- An optimal deterministic policy always exists for any Markov Decision Process (MDP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration-Exploitation Dilemma\n",
    "\n",
    "- The exploration-Exploitation dilemma refers to the need to both collect rewards from good known combinations of actions and states, and keep trying new actions to see if they can produce yet better rewards.\n",
    "- Stochastic policies can be used for **Exploration**, or in settings where transition probabilities of a MDP are unknown.\n",
    "    N.B. Such dilemma is only relevant to online-RL, when agent interact with the env in real time.\n",
    "- In batch-mode RL, we already have data collected by other agent that we can later utilize.\n",
    "\n",
    "\n",
    "### Value Function and Bellman Equation\n",
    "\n",
    "- For time-homogeneous MDPs, transition probabilities and rewards do not depend on time, and the time horizon is infinite. In such cases, the value fn is also independent on time.\n",
    "- Bellman eq for a time-indpendent value fn is equivalent to a system of $N$ linear equations, where $N$ is the number of discrete states.\n",
    "\n",
    "- The value fn depends on the current state and the policy used to collect rewards.\n",
    "- Bellman Optimality is a non-linear equation that generally needs to be solved numerically. This is mainly because of the max operator.\n",
    "- The optimal value fn is a maximum of all possible value fn for different policies among all possible policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration and Policy Iteration\n",
    "\n",
    "- For VI, there are 2 ways to update:\n",
    "\t- Sync: where you need to wait till the end of iteration to make one update for all states at once.\n",
    "\t- ASync: updates VI on the fly.\n",
    "\n",
    "**N.B.** In both cases, convergence is guaranteed to the optimal Value Fn, as long as state-action is discrete.\n",
    "\n",
    "\n",
    "- Optimal policy calculation in Value Iteration is computationally the same as policy iteration step in Policy Iteration method.\n",
    "\n",
    "\n",
    "### Action-Value Function\n",
    "\n",
    "- Value Fn depends only on the current state, does NOT tell the agent what it should do.\n",
    "- Action-Value Fn takes action a, then follows policy pi. It is a fn of both current state and action.\n",
    "\n",
    "**N.B.** First action a is fixed.\n",
    "\n",
    "\n",
    "- **Optimal Q-Fn**: take action a now, then follows optimal policy pi.\n",
    "- **Optimal Value-Fn**: take optimal action a_star, as determined by optimal policy both now and later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from Experience\n",
    "\n",
    "- Both state value and action-state value fn can be estimated from experience.\n",
    "- An agent can follow some policy $\\pi$, and compute the average reward for each visited state. This average would converge to the value fn at this state.\n",
    "- If averages are computed seperately for each action taken, we can estimate the action-state value fn in the same way.\n",
    "- Estimations of this kind are called **Monte Carlo Methods**.\n",
    "- If there are many states, it may be not practical to keep seperate averages for each state - need to rely on Function Approximations.\n",
    "\n",
    "\n",
    "**Note**: \n",
    "- An optimal value fn is obtained by maximization of a Q-fn wrt action variable, NOT state.\n",
    "- An Q- fn is a fn of a current state and current action, unlike a value fn that is only a fn of current state.\n",
    "- MC directly estimate the Q-fn by summing observed rewards along trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "## Part 2: Discrete Time Black-Scholes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black-Scholes-Merton Model\n",
    "\n",
    "- It is the basis of modern quantitative finance.\n",
    "- More about it on [investopedia](https://www.investopedia.com/terms/b/blackscholes.asp)\n",
    "\n",
    "\n",
    "- The model is mainly used for **pricing an options contract**. It utilizes 5 inputs: *underlying asset*, *strike price of the option*, *volatility*, *time till expiration of the option*, and finally *risk-free interest rate*.\n",
    "\n",
    "\n",
    "- In addition, it also predicts that the **price of heavily traded assets** follows a geometric Brownian Motion with constant drift and volatility. When applied to a stock option, the model incorporates the constant price variation of the stock, the time value of money, the options's stirke price, and the time to the option's expiry.\n",
    "\n",
    "\n",
    "- **Law of One Price**: if 2 securities pay the same in all possible future states, they should have the same price now. \n",
    "\n",
    "- Idea of BSM is:\n",
    "\t- building a total portfolio, consisting of a replicating portfolio and short-position in the stock cancelling each other out. **Zero-Wealth Portfolio**.\n",
    "\t- The price obtained by this modelling is called an **Equilibrium Price**.\n",
    "\n",
    "\n",
    "- **Financial options** are particular sort of financial derivatives whose value is derived from the value of underlying assets (e.g stocks).\n",
    "- A European Put option gives a seller of this option a right, but not an obligation, to sell a stock underlying this option, for a pre-specified price (strike) $K$ at some future time $T$.\n",
    "- The profit for a buyer of a call option is equal to $max (S_t - K, 0)$.\n",
    "\n",
    "\n",
    "**Note** : BSM finds a perfect replicating portfolio whose price is always equal to the option price, no matter what the stock price goes in the future.\n",
    "\n",
    "\n",
    "- When it comes the number of stocks that should be traded, BSM mimics the option movements by frequently shuffling the money between the stock investment and the bank cash amount. BSM ensures that the total portfolio value is *always* zero in the future, no matter how the stock behaves.\n",
    "- This is only possible, if the time steps are infintesimal. For such very special setting, BSM finds a *unique* option price and *unique* number of shares one should in the replicating portfolio. **Pricing by Hedging**.\n",
    "\n",
    "- Stock dynamics in BSM follows the Law of Geometric Brownian Motion with a drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BSM Model and Risk\n",
    "\n",
    "- BSM, however, creates a paradoxical conclusion that options are useless, since they could replicated in such way. BSM completely remove the risk; in favor of tracktability. The assumption is: *zero transaction costs and continuous hedging*.\n",
    "\n",
    "- In practice however, options are worthy, since they carry a substantial risk. In practice, nobody trades options at BSM pricing. The difference reflects the amount of underlying risk embedded in trading options.\n",
    "\n",
    "\n",
    "- The purpose of **Replicating Portfolio** is to dynamically track option value in different state of the world, so that a *total portfolio* made of the stock and the replication portfolio has a zero (or close to zero) value.\n",
    "\n",
    "- A Replicating Portfolio for an option is made of a stock and cash.\n",
    "- The law of One Price states that securities that have the same value in all states of the world should also have the same price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Time BSM\n",
    "\n",
    "- The **discretization** allows for a more realistic implementation of BSM, instead of the continuous implementation of BSM orginally suggested.\n",
    "\n",
    "- Such discretization shifts the focus to risk minimization; conducted by hedging in a sequential decision making process.\n",
    "\n",
    "- Implementation: providing the agent with history of the market and trading strategy; aiming to improve goals. We can do that with Q-Learning, where the agent learns to produce optimal strategy even when trained on suboptimal policy.\n",
    "\n",
    "\n",
    "- The problem we are trying to solve is **Sequential Risk Minimization**.\n",
    "- The details of such implementation can be found under: \"RL-week-1-2-4-Discrete-time-BSM\". Available on Coursera.\n",
    "\n",
    "\n",
    "- The value of the hedge portfolio at time T is equal to the option price at time $T$, i.e the option payoff.\n",
    "- A forward pass should be done only *once*, as for a small investor assumed in the option pricing problem, a hedge strategy does not impact the market. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedging and Pricing\n",
    "\n",
    "- Computing Optimal Hedges requires looking at all MC paths simultaneously, also called **Cross-Sectional Analysis**.\n",
    "\n",
    "**N.B.** MC used previously mainly gives us value a certain state, yet we need the whole policy.\n",
    "\n",
    "- We shall work backward in time. That's because we do not know the future values of states, hence we are going to condition on the current state\n",
    "- The details of such implementation can be found under: RL-week-1-2-5-BSM_Hedging_Limit.\n",
    "\n",
    "\n",
    "- The optimal hedge $u_t$ should minimize the one-step variance of the option replicating portfolio $Pi_t$.\n",
    "- Because the optimization problem for finding the optimal hedge $u_t$ is quadratic, it can be solved semi-analytically.\n",
    "\n",
    "\n",
    "- The optimal hedge $u_t$ depends only on the current value of the stock price."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}



some notes..




(Dynamic Programming)

- It is a collection of alg that can be used to compute optimal policies given a perfect model of the env as a MDP.
- Running assumption: finite MDP.
- Although DP ideas can be applied to problems with continuous state & action spaces, exact solutions are possible only in special cases.


- The key idea of DP, and of RL generally, is the use of value fn to organize the search for good policies.
- Optimal policies satisfying Bellman Optimality Eq(4.1) & (4.2).




(4.1 Policy Evaluation)

- Policy Evaluation is the idea of computing the state-value fn v_pi for an arbitrary policy_pi. This is also called "Prediction Problem". Eq(4.3) & (4.4).
- The existence and uniqueness of this v_pi are guaranteed as long as either gamma<1 or eventual termination is guaranteed from all states under the policy pi.
- To solve such system of eq, the initial appr v_0 is chosen arbitrarily (except the termination state must be given value 0) and we iteratively update the rest of the states using Bellman Eq as update rule eq(4.5)
- Such alg is called Iterative Policy Evaluation, as the sequence can be shown in general to converge to v_pi as k tends to infinity.


- To produce each successive appr v_k+1 from v_k, Iterative Policy Evaluation applies the same operation to each state s: it replacecs the old values of s with a new value obtained from the old values of the successor states of s, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated. This is called "Full Backup".
- Each iteration of IPE backs up the value of every state once to produce the new appr value fn v_k+1.
N.B. All the backups done in DP are called full backups because they are based on all possible next states rather than on a sample next state.


- Practically, a sequential computer program can be written using 2 arrays; one for the old values and one for the new values. Of couse, it would be easier to use only one array and update the values "in place", where the alg usually converges faster than the 2-array version.
- We think of the backups as being done in a sweep through the state space. For the in-place alg, the order in which states are backed up during the sweep has a significant influence on the rate of the convergence. Fig(4.1)




(4.2 Policy Improvement)

- Again, the reason for computing the value fn for a policy is to help find better policies.
- Suppose we have determined the value fn v_pi for an arbitrary deteministic policy pi. We would like to know whether or not we should change the policy to determinstically choose an action a != pi(s). In other words, would it be better to change the current policy pi? Eq(4.6)
- The key criterion is whether this is greater than or less than v_pi(s). This idea is called Policy Improvement Theorem.


- Naturally, we can extend this to consider changes at all states and to all possible actions. Greedy Policy eq(4.9)
- The greedy policy takes the action that looks best in the short term -after one step of lookahead- according to v_pi.
- The process of making a new policy that improves on an original policy, by making it greedy wrt the value fn of the original policy, is called Policy Improvement.


- So far, we have only considered the special case of determinsitc policies. The ideas can easily extend to stochastic policies.




(4.3 Policy Iteration)

- Following the ideas of evaluation and imrpovement, we can thus obtain a sequence of monotonically improving policies and value fn, where each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Fig(4.3)
- Again under our assumption of finite MDP, this process must converge to an optimal policy and optimal value fn in a finite #iterations.
- Such approach is called "Policy Iteration".


- N.B. each policy evaluation, itself an iterative computation, is started with the value fn for the previous policy. This typically results in a great increase in the speed of convergence of policy evaluation (presumably because the value fn changes little from one policy to the next).
- PI often converges in surprisingly few iterations.


- One major drawback of PI is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to v_pi occurs only in the limit. Fig(4.2) suggests that it may be possible to truncate policy evaluation.




(4.4 Value Iteration)

- Due to the above-mentioned drawback for PI, the policy evaluation step of PI can actually be truncated in several ways without losing the convergence guarantees of PI.
- One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This alg is called Value Iteration. It can be written as a particularly simple backup operation that combines the policy improvement and truncated policy evaluation. eq(4.10)


- Another way of understanding CI is by reference to Bellman Optimality eq(4.1).  VI is obtained simply by turning the Bellman Optimality eq into an update rule.
N.B. VI backup is identical to policy evaluation backup eq(4.5), except that it requires the maximum to be taken over all actions.


- To terminate, VI -like PI- requires an inifinite #iterations to converge exactly to v_star. In pratice however, one can stop once the value fn changes by only a small amount in a sweep. Fig(4.5)


- VI effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement.
- Faster comvergence is often achieved by interposing multiple policy evaluation sweeps bet each policy improvement sweep. i.e. evaluate a lot before improving.


N.B. In general, the entire clasee of "Truncated PI" alg can be thought of as sequence of sweeps, some of which use policy evaluation backups and some of which use VI backups.
Since only the max operation in (4.10) is the only difference bet these backups, this just means that the max operation is added to some sweeps of policy evaluation.

- All of these alg converge to an optimal policy for discounted finite MDPs.




(4.5 Asynchronous DP)

- Major drawback to DP methods discussed so far is that they involve operations over the entire state set of the MDP, that is, they require sweeps of the state sets. Obviously, this is NOT practical.
- Async DP are in-place iterative DP alg that are not organized in terms of systematic sweeps of the state set. These alg back up the values of states in any order whatsoever, using whatever values other states happen to be available.
Values of some states may be backed up several times before the values of others are backed up once.


- To converge correctly, however, an async alg must continue to backup the values of all the stages: it can't ignore any state after some point in the computation. Async DP alg allow great flexibility in selecting to which backup operations are applied.


- One version of Async VI backs up the value, in place, of only one state s_k, on each step k, using the VI backup(4.10). If 0 <= gamma < 1, asymptotic convergence to v_star is guaranteed given only that all states occur in the sequence {s_k} an infinite #times (the seq could even be stochastic).
- Similarly, it is possible to intermix policy evaluation and VI backups to produce a kind of Async truncated PI.


- It is important to note that avoiding sweeps does not necessarily mean that we can get away with less computation. It just means that an alg does not need to get locked into any hopelessly long sweep before it can make progress improving a policy.
- In order to take advantage of this flexilibity, we can select the states to which we apply backups so as to improve the alg's rate of progress. We can try to order the backups to let value info propagate from state to state in an efficient way. We cab even try skipping backing up some state entirely.


- The real advantage of Async DP is that it makes it easier to intermix computation with real-time interaction. To solve a given MDP, we can run an iterative DP alg at the same time that an agent is actually experienceing the MDP. For example, we can apply backups to states as the agent visits them.
- Doing so, makes it possible to focus the DP alg's backups onto parts of the state set that are most relevant to the agent.




(4.6 Generalized Policy Iteration)

- PI:
	- it consists of 2 simultaneous, interacting processes, one making the value fn consistent with the current policy (Policy 		  Evaluation) and the other making policy greedy wrt the current value fn (Policy Imrpovement).
	- the 2 processes alternate, each completing before the other begins. This is NOT really necessarily.

- VI:
	- for example, only a single iteration of policy evaluation is performed in bet each policy improvement.

- Async DP:
	- both operations: evaluation & improvement. are interleaved at an even finer grain. In some cases a single state is updated in one 		  process before returning to the other.
	- As long as both processes continue to update all states, the ultimate result is typically the same - convergence to the optimal 		  value fn and an optimal policy.


- GPI refer to the general idea of letting policy evaluation and policy improvement processes interact, indepedent of the granularity and other details of the 2 processes. Almost all RL methods can be described as GPI.
- That is, all have identifiable policies and value fn, with the policy always being imrpoved wrt the value fn and the value fn always being drive towards the value fn for the policy. Fig(4.7)


- In case both processes stabilize, both value fn and policy must be optimal.
The value fn stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy wrt the current value fn. This implies that the Bellman Optimality Eq(4.1) holds.


- The evaluation and improvement processes in GPI can be viewed as both competing and cooperating. They compete in the sense that they pull in opposing directions. In the long term, however, they interact to find a single joint solution.




(4.7 Efficieny of DP)

- Compared with other methods for solving MDPs, DP methods are actually quite efficient. The (worse case) time DP methods take to find an optimal policy is polynomial in #states and actions.
- DP is guaranteed to find an optimal policy in polynomial time even thoug the total # determinstic policies in m**n (m: #actions, n: #states).


- DP may not be practical for very large problems, mainly due to curse of dimensionality. 
- Asynch DP would be preffered in case of large state spaces.


- Practically, DP methods can be used with today's computers to solve MDPs with millions of states. Both PI & VI are widely used, and it is still unclear which, if either, is better in general.




(4.8 Summary)

- It is NOT necessary to perform DP in complete sweeps through the state set. Async DP are in-place iterative methods that back up states in an arbitrary order, perhaps stochastically determined and using out-of-date info. Many of such methods can be viewed as fine-grained forms of GPI.

- All DP methods update estimates of the values of states based on estimate of the values of successor states. That is, they update estimates on the basis of other estimates. This general idea is called BOOTSTRAPPING.















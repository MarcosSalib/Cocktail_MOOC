{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing and Word Embeddings\n",
    "\n",
    "Welcome to this new exercise! In this exercise, we will play around with text instead of images as before, using Recurrent Neural Networks. Generally it is called Natural Language Processing (NLP) when dealing with text, speech, etc. But the data structure is very different to images, i.e. text is string instead of numbers in images. So we need some preprocessing steps to transform raw text to other data format. And this notebook will introduce these basic concepts in NLP pipelines. Specifically, you will learn about:\n",
    "\n",
    "1. How to preprocess text classification datasets\n",
    "2. How to create a simple word embedding layer that maps words to dense vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "As usual, we first import some packages to setup this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from exercise_code.rnn.sentiment_dataset import (\n",
    "    create_dummy_data,\n",
    "    download_data\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing a Text Classification Dataset\n",
    "\n",
    "As a starting point, let's load a dummy text classification dataset and have a sense how it looks like. We take these samples from the IMDb movie review dataset, which includes movie reviews and labels that show whether they are negative (0) or positive (1). You will investigate this task further in the second notebook.\n",
    "\n",
    "In this section, our goal is to create a text processing dataset. You are not required to write any code in this section. However, the concept introduced here is very important for working on NLP datasets in the future as well as in the rest of this exercise. So take your time to understand the procedure here. :)\n",
    "\n",
    "First, let us download the data and take a look at some data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"SentimentData\")\n",
    "\n",
    "path = download_data(data_root)\n",
    "data = create_dummy_data(path) # check sentiment_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://vision.in.tum.de/webshare/g/i2dl/SentimentData.zip to /mnt/RCI/I2DL_recap/week_11/datasets/SentimentData/SentimentData.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3776512it [00:09, 415122.85it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This is the definitive movie version of Hamlet. Branagh cuts nothing, but there are no wasted moments.\n",
      "Label: 1\n",
      "\n",
      "Text: Adrian Pasdar is excellent is this film. He makes a fascinating woman.\n",
      "Label: 1\n",
      "\n",
      "Text: I don't know why I like this movie so well, but I never get tired of watching it.\n",
      "Label: 1\n",
      "\n",
      "Text: Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.\n",
      "Label: 0\n",
      "\n",
      "Text: The characters are unlikeable and the script is awful. It's a waste of the talents of Deneuve and Auteuil.\n",
      "Label: 0\n",
      "\n",
      "Text: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text, label in data:\n",
    "    print('Text: {}'.format(text))\n",
    "    print('Label: {}'.format(label))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tokenizing Data\n",
    "\n",
    "As seen above, we loaded 3 positive and 3 negative reviews. Since the basic semantic unit of text is word, the first thing we need to do is **tokenizing** the dataset, which means converting each review to a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['this', 'is', 'the', 'definitive', 'movie', 'version', 'of', 'hamlet', 'branagh', 'cuts', 'nothing', 'but', 'there', 'are', 'no', 'wasted', 'moments'], 1) \n",
      "\n",
      "(['adrian', 'pasdar', 'is', 'excellent', 'is', 'this', 'film', 'he', 'makes', 'a', 'fascinating', 'woman'], 1) \n",
      "\n",
      "(['i', 'don', 't', 'know', 'why', 'i', 'like', 'this', 'movie', 'so', 'well', 'but', 'i', 'never', 'get', 'tired', 'of', 'watching', 'it'], 1) \n",
      "\n",
      "(['great', 'movie', 'especially', 'the', 'music', 'etta', 'james', 'at', 'last', 'this', 'speaks', 'volumes', 'when', 'you', 'have', 'finally', 'found', 'that', 'special', 'someone'], 0) \n",
      "\n",
      "(['the', 'characters', 'are', 'unlikeable', 'and', 'the', 'script', 'is', 'awful', 'it', 's', 'a', 'waste', 'of', 'the', 'talents', 'of', 'deneuve', 'and', 'auteuil'], 0) \n",
      "\n",
      "(['no', 'comment', 'stupid', 'movie', 'acting', 'average', 'or', 'worse', 'screenplay', 'no', 'sense', 'at', 'all', 'skip', 'it'], 0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# use regular expression to split the sentence\n",
    "# check https://docs.python.org/3/library/re.html for more information\n",
    "def tokenize(text):\n",
    "    return [s.lower() for s in re.split(r'\\W+', text) if len(s) > 0]\n",
    "\n",
    "tokenized_data = []\n",
    "for text, label in data:\n",
    "    tokenized_data.append((tokenize(text), label))\n",
    "    print(tokenized_data[-1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Creating a Vocabulary\n",
    "\n",
    "We have converted the dataset into pairs of token lists and corresponding labels. But strings have variant length and are less easy for handling. It would be nice to represent words with numbers. So, we need to create a <b>vocabulary</b>, which is a dictionary that maps each word to an integer id.\n",
    "\n",
    "In large datasets, there are too many words and most of them don't occur very frequently. One common approach we use to tackle this problem is to pick most common N words from the dataset. Therefore, we restrict the number of words.\n",
    "\n",
    "Let's first compute the word frequencies in our dummy dataset. To compute frequencies, we use the [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'this': 4,\n",
       "         'is': 4,\n",
       "         'the': 5,\n",
       "         'definitive': 1,\n",
       "         'movie': 4,\n",
       "         'version': 1,\n",
       "         'of': 4,\n",
       "         'hamlet': 1,\n",
       "         'branagh': 1,\n",
       "         'cuts': 1,\n",
       "         'nothing': 1,\n",
       "         'but': 2,\n",
       "         'there': 1,\n",
       "         'are': 2,\n",
       "         'no': 3,\n",
       "         'wasted': 1,\n",
       "         'moments': 1,\n",
       "         'adrian': 1,\n",
       "         'pasdar': 1,\n",
       "         'excellent': 1,\n",
       "         'film': 1,\n",
       "         'he': 1,\n",
       "         'makes': 1,\n",
       "         'a': 2,\n",
       "         'fascinating': 1,\n",
       "         'woman': 1,\n",
       "         'i': 3,\n",
       "         'don': 1,\n",
       "         't': 1,\n",
       "         'know': 1,\n",
       "         'why': 1,\n",
       "         'like': 1,\n",
       "         'so': 1,\n",
       "         'well': 1,\n",
       "         'never': 1,\n",
       "         'get': 1,\n",
       "         'tired': 1,\n",
       "         'watching': 1,\n",
       "         'it': 3,\n",
       "         'great': 1,\n",
       "         'especially': 1,\n",
       "         'music': 1,\n",
       "         'etta': 1,\n",
       "         'james': 1,\n",
       "         'at': 2,\n",
       "         'last': 1,\n",
       "         'speaks': 1,\n",
       "         'volumes': 1,\n",
       "         'when': 1,\n",
       "         'you': 1,\n",
       "         'have': 1,\n",
       "         'finally': 1,\n",
       "         'found': 1,\n",
       "         'that': 1,\n",
       "         'special': 1,\n",
       "         'someone': 1,\n",
       "         'characters': 1,\n",
       "         'unlikeable': 1,\n",
       "         'and': 2,\n",
       "         'script': 1,\n",
       "         'awful': 1,\n",
       "         's': 1,\n",
       "         'waste': 1,\n",
       "         'talents': 1,\n",
       "         'deneuve': 1,\n",
       "         'auteuil': 1,\n",
       "         'comment': 1,\n",
       "         'stupid': 1,\n",
       "         'acting': 1,\n",
       "         'average': 1,\n",
       "         'or': 1,\n",
       "         'worse': 1,\n",
       "         'screenplay': 1,\n",
       "         'sense': 1,\n",
       "         'all': 1,\n",
       "         'skip': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freqs = Counter()\n",
    "for tokens, _ in tokenized_data:\n",
    "    freqs.update(tokens)\n",
    "\n",
    "freqs # this is dict mappying words to their frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dictionary, let's select the most common 20 words to create a vocabulary. In addition to the words that appear in our data, we need to have two special words:\n",
    "\n",
    "- `<eos>` End of sequence symbol used for padding\n",
    "- `<unk>` Words unknown in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<eos>': 0,\n",
       " '<unk>': 1,\n",
       " 'the': 2,\n",
       " 'this': 3,\n",
       " 'is': 4,\n",
       " 'movie': 5,\n",
       " 'of': 6,\n",
       " 'no': 7,\n",
       " 'i': 8,\n",
       " 'it': 9,\n",
       " 'but': 10,\n",
       " 'are': 11,\n",
       " 'a': 12,\n",
       " 'at': 13,\n",
       " 'and': 14,\n",
       " 'definitive': 15,\n",
       " 'version': 16,\n",
       " 'hamlet': 17,\n",
       " 'branagh': 18,\n",
       " 'cuts': 19,\n",
       " 'nothing': 20,\n",
       " 'there': 21}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {'<eos>': 0, '<unk>': 1}\n",
    "for token, freq in freqs.most_common(20):\n",
    "    vocab[token] = len(vocab)\n",
    "vocab # dict mapping top 20 most common to id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating the Dataset\n",
    "\n",
    "Putting it all together, we can now create a dataset class. First, let's create index-label pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 2, 15, 5, 16, 6, 17, 18, 19, 20, 10, 21, 11, 7, 1, 1]  ->  1\n",
      "\n",
      "[1, 1, 4, 1, 4, 3, 1, 1, 1, 12, 1, 1]  ->  1\n",
      "\n",
      "[8, 1, 1, 1, 1, 8, 1, 3, 5, 1, 1, 10, 8, 1, 1, 1, 6, 1, 9]  ->  1\n",
      "\n",
      "[1, 5, 1, 2, 1, 1, 1, 13, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  ->  0\n",
      "\n",
      "[2, 1, 11, 1, 14, 2, 1, 4, 1, 9, 1, 12, 1, 6, 2, 1, 6, 1, 14, 1]  ->  0\n",
      "\n",
      "[7, 1, 1, 5, 1, 1, 1, 1, 1, 7, 1, 13, 1, 1, 9]  ->  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_data = []\n",
    "for tokens, label in tokenized_data:\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]    \n",
    "    # the token that is not in vocab get assigned <unk>\n",
    "    indexed_data.append((indices, label))\n",
    "    \n",
    "for indices, label in indexed_data:\n",
    "    print(indices, ' -> ', label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([3, 4, 2, 15, 5, 16, 6, 17, 18, 19, 20, 10, 21, 11, 7, 1, 1], 1)\n",
      "(['this', 'is', 'the', 'definitive', 'movie', 'version', 'of', 'hamlet', 'branagh', 'cuts', 'nothing', 'but', 'there', 'are', 'no', 'wasted', 'moments'], 1)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(indexed_data)))\n",
    "print(next(iter(tokenized_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>We now use the PyTorch dataset class we provided in <code>exercise_code/rnn/sentiment_dataset.py</code> file. Please also take a look at the code.</p>\n",
    " </div>\n",
    "    \n",
    "\n",
    "\n",
    "Dataset class also reverse sorts the sequences with respect to the lengths. Thanks to this sorting, we can reduce the total number of padded elements, which means that we have less computations for padded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': tensor([ 1,  5,  1,  2,  1,  1,  1, 13,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1]), 'label': tensor(0.)}\n",
      "\n",
      "{'data': tensor([ 2,  1, 11,  1, 14,  2,  1,  4,  1,  9,  1, 12,  1,  6,  2,  1,  6,  1,\n",
      "        14,  1]), 'label': tensor(0.)}\n",
      "\n",
      "{'data': tensor([ 8,  1,  1,  1,  1,  8,  1,  3,  5,  1,  1, 10,  8,  1,  1,  1,  6,  1,\n",
      "         9]), 'label': tensor(1.)}\n",
      "\n",
      "{'data': tensor([ 3,  4,  2, 15,  5, 16,  6, 17, 18, 19, 20, 10, 21, 11,  7,  1,  1]), 'label': tensor(1.)}\n",
      "\n",
      "{'data': tensor([ 7,  1,  1,  5,  1,  1,  1,  1,  1,  7,  1, 13,  1,  1,  9]), 'label': tensor(0.)}\n",
      "\n",
      "{'data': tensor([ 1,  1,  4,  1,  4,  3,  1,  1,  1, 12,  1,  1]), 'label': tensor(1.)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.rnn.sentiment_dataset import SentimentDataset\n",
    "\n",
    "combined_data = [\n",
    "    (raw_text, tokens, indices, label)\n",
    "    for (raw_text, label), (tokens, _), (indices, _)\n",
    "    in zip(data, tokenized_data, indexed_data)\n",
    "]\n",
    "\n",
    "dataset = SentimentDataset(combined_data)\n",
    "\n",
    "for elem in dataset:\n",
    "    print(elem)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Minibatching\n",
    "Note that in the dataset we created, not all sequences have the same length. Therefore, we cannot minibatch the data trivially. This means we cannot use a `DataLoader` class easily.\n",
    "\n",
    "<b>If you uncomment the following cell and run it, you will very likely get an error!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset, batch_size=3)\n",
    "\n",
    "# for batch in loader:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>To solve the problem, we need to pad the sequences with <code> < eos > </code> tokens that we indexed as zero. To integrate this approach into the Pytorch <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\" target=\"_blank\">Dataloader</a> class, we will make use of the <code>collate_fn</code> argument. For more details, check out the <code>collate</code> function in <code>exercise_code/rnn/sentiment_dataset</code>. </p>\n",
    "    <p> In addition, we use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\" target=\"_blank\">pad_sequence</a> that pads shorter sequences with 0. </p>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \n",
      " tensor([[ 1,  2,  8],\n",
      "        [ 5,  1,  1],\n",
      "        [ 1, 11,  1],\n",
      "        [ 2,  1,  1],\n",
      "        [ 1, 14,  1],\n",
      "        [ 1,  2,  8],\n",
      "        [ 1,  1,  1],\n",
      "        [13,  4,  3],\n",
      "        [ 1,  1,  5],\n",
      "        [ 3,  9,  1],\n",
      "        [ 1,  1,  1],\n",
      "        [ 1, 12, 10],\n",
      "        [ 1,  1,  8],\n",
      "        [ 1,  6,  1],\n",
      "        [ 1,  2,  1],\n",
      "        [ 1,  1,  1],\n",
      "        [ 1,  6,  6],\n",
      "        [ 1,  1,  1],\n",
      "        [ 1, 14,  9],\n",
      "        [ 1,  1,  0]])\n",
      "\n",
      "Labels: \n",
      " tensor([0., 0., 1.])\n",
      "\n",
      "Sequence Lengths: \n",
      " tensor([20, 20, 19])\n",
      "\n",
      "\n",
      "Data: \n",
      " tensor([[ 3,  7,  1],\n",
      "        [ 4,  1,  1],\n",
      "        [ 2,  1,  4],\n",
      "        [15,  5,  1],\n",
      "        [ 5,  1,  4],\n",
      "        [16,  1,  3],\n",
      "        [ 6,  1,  1],\n",
      "        [17,  1,  1],\n",
      "        [18,  1,  1],\n",
      "        [19,  7, 12],\n",
      "        [20,  1,  1],\n",
      "        [10, 13,  1],\n",
      "        [21,  1,  0],\n",
      "        [11,  1,  0],\n",
      "        [ 7,  9,  0],\n",
      "        [ 1,  0,  0],\n",
      "        [ 1,  0,  0]])\n",
      "\n",
      "Labels: \n",
      " tensor([1., 0., 1.])\n",
      "\n",
      "Sequence Lengths: \n",
      " tensor([17, 15, 12])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(batch):\n",
    "    assert isinstance(batch, list)\n",
    "    data = pad_sequence([b['data'] for b in batch])\n",
    "    lengths = torch.tensor([len(b['data']) for b in batch])\n",
    "    label = torch.stack([b['label'] for b in batch])\n",
    "    return {\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'lengths': lengths\n",
    "    }\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=3, collate_fn=collate)\n",
    "for batch in loader:\n",
    "    print('Data: \\n', batch['data'])\n",
    "    print('\\nLabels: \\n', batch['label'])\n",
    "    print('\\nSequence Lengths: \\n', batch['lengths'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these two batches have different length, this is how the reverse sort mentioned in `1.3 Creating the Dataset` benefits for less memory and less computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeddings\n",
    "\n",
    "In the previous section, we explored how to convert text into a sequence of integers. In this form, sequences are still not ready to be inputs of RNNs you implemented in the optional notebook. Integer representation is some kind of one-hot encoding, while not the same since they are not equally weighted given only an integer. \n",
    "\n",
    "Moreover, it fails to express the semantic relations between words and the order of the words has no meaning. We would like a better representation form to keep semantic meaning of the word. For example, as shown in the following picture, the difference between man and woman and difference between king and queen should be close, since the difference is only the gender. If we use a vector for each word, the above relation can be expressed as $vec(\\text{women})-vec(\\text{man}) \\approx vec(\\text{queen}) - vec(\\text{king})$. Usually we call such vector representations as embeddings.\n",
    "\n",
    "<img src='https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg' width=80% height=80%/>\n",
    "\n",
    "While one can use pre-trained embedding vectors such as [word2vec](https://arxiv.org/abs/1301.3781) or [GLoVe](https://nlp.stanford.edu/projects/glove/), in this exercise we use randomly initialized embedding vectors that will be trained from scratch together with our networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h3> Task: Implement Embedding</h3>\n",
    " <p>In this part, you will implement a simple embedding layer. Embedding is a simple lookup table that stores a dense vector to represent each word in the vocabulary.</p> \n",
    "\n",
    " <p>Your task is to implement the <code>Embedding</code> class in <code>exercise_code.rnn.rnn_nn</code> file. Once you are done, run the below cell to test your implementation. Note that we ensure eos embeddings to be zero by using the <code>padding_idx</code> argument.\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between outputs: 0.0\n",
      "Test passed :)!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from exercise_code.rnn.rnn_nn import Embedding\n",
    "from exercise_code.rnn.tests import embedding_output_test\n",
    "\n",
    "\n",
    "i2dl_embedding = Embedding(len(vocab), 16, padding_idx=0)\n",
    "pytorch_embedding = nn.Embedding(len(vocab), 16, padding_idx=0)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), collate_fn=collate)\n",
    "for batch in loader:\n",
    "    x = batch['data']\n",
    "\n",
    "embedding_output_test(i2dl_embedding, pytorch_embedding, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "In this notebook, you learned how to prepare text data and how to create an embedding layer. In the next notebook, you will combine your Embedding and RNN implementations to create a sentiment analysis network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "i2dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
